# config.yaml
# WITS-NEXUS v2 Configuration

# General Settings
internet_access: false
allow_code_execution: false # Default to false for safety
ethics_enabled: true
output_directory: "output" # Relative to project root
debug_mode: true # Enable debug mode
skip_llm_initialization: true # Skip LLM initialization for testing

# Voice Input Settings (Optional)
voice_input: false
voice_input_duration: 5 # seconds
whisper_model: "base"
whisper_fp16: false

# Ollama Model Configurations
models:
  default: "llama3" # General purpose default
  orchestrator: "openhermes:latest" # For the main orchestrator agent
  scribe: "llama3"
  analyst: "openhermes:latest"
  engineer: "codellama:7b"
  researcher: "llama3"
  coder: "codellama:7b"
  # Add other specialized agent models if needed

# Router Configuration (Orchestrator is usually the primary handler)
router:
  fallback_agent: "orchestrator_agent" # Key for your orchestrator agent instance
  

# Web Interface Settings (Flask/FastAPI)
web_interface:
  enabled: true # Set to true to run the web app by default with run.py
  port: 5001
  host: "0.0.0.0"
  debug: true # Flask/FastAPI debug mode
  enable_file_uploads: true
  max_file_size: 10 # In MB
  allowed_file_types: ["text/plain", "application/json", "application/yaml", "text/yaml"]
  file_upload_directory: "uploads" # Directory to save uploaded files
  


ollama_url: "http://localhost:11434"
llm_model_name: "llama3:latest" # Default model
embedding_model_name: "all-MiniLM-L6-v2"
vector_db_path: "data/vector_db"
ollama_request_timeout: 120
max_iterations: 10 # Increased for web interaction
file_tool_base_path: "data/user_files" # Example base path for FileTool

# Orchestrator Specific Settings
orchestrator_max_iterations: 15 # Max steps in a ReAct loop for a single goal

# Any other core component settings
# memory_manager:
#   vector_model: "all-MiniLM-L6-v2" # If using vector memory

# ollama_options: # Global options for ollama.generate, if any
#   temperature: 0.7
#   num_ctx: 4096

# Agent Profiles
agent_profiles:
  general_orchestrator:
    agent_class: "agents.orchestrator_agent.OrchestratorAgent"
    llm_model_name: "openhermes:latest" # Corrected from model
    temperature: 0.7
    tool_names:
      - "calculator"
      - "datetime"
      - "web_search" # If internet_access is true
      - "read_file"
      - "write_file"
      - "list_files"
    max_iterations: 15
    # You can add a system_prompt_override here if needed
    # system_prompt_override: "You are a helpful general assistant."
  engineer:
    agent_class: "agents.specialized.engineer_agent.EngineerAgent"
    llm_model_name: "codellama:7b" # Corrected from model
    temperature: 0.2  # Lower temperature for more precise coding tasks
    tool_names: 
      - "project_file_reader"
      - "git_tool"
    max_iterations: 10
    ollama_options:
      num_ctx: 8192  # Larger context for code analysis

# Tool Configurations
tool_configs:
  project_file_reader:
    allowed_extensions: [".py", ".md", ".txt", ".yaml", ".yml", ".json"]
    exclude_patterns: ["__pycache__", "*.pyc", "*.pyo", "*.pyd"]
  
  git_tool:
    allowed_commands: ["status", "branch", "add", "commit", "diff", "log"]
    require_confirmation: true  # Whether to require human confirmation for git operations
    branch_prefix: "wits-improvement-"  # Prefix for auto-generated branch names

# Debug Settings
debug:
  enabled: true
  log_level: "DEBUG"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  console_logging_enabled: true
  console_log_level: "INFO"
  file_logging_enabled: true
  log_directory: "logs"
  performance_monitoring: true
  components:
    llm_interface:
      log_prompts: true
      log_responses: true
      log_tokens: true
    memory_manager:
      log_embeddings: true
      log_searches: true
    tools:
      log_args: true
      log_results: true
    agents:
      log_thoughts: true
      log_actions: true
      log_delegations: true
