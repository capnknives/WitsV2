# core/llm_interface.py
# Behold! The magical bridge to our AI friends! ^_^
import ollama  # Our trusty companion in the AI adventure! \o/
import time
from typing import Any, Dict, Optional, Union, List
import json
from datetime import datetime
import logging
from pydantic import ValidationError

from .schemas import OrchestratorLLMResponse  # For keeping our responses neat and tidy =D
from .debug_utils import DebugInfo, log_debug_info  # Debug powers, activate! x.x
from .json_utils import safe_json_loads # Corrected import

class LLMInterface:
    """
    Your friendly AI language model interface! =D
    
    I help you chat with those fancy Ollama LLMs:
    1. Basic chitchat (the simple stuff ^_^)
    2. Fancy JSON outputs (gotta keep it structured \\o/)
    3. Fixing oopsies (retry all the things! >.>)
    4. Watching everything like a hawk (debug mode activated! x.x)
    """
    
    def __init__(
        self,
        model_name: Optional[str] = None,  # Which AI friend should we talk to? =P
        temperature: float = 0.7,  # How spicy should our responses be? ^_^
        ollama_url: Optional[str] = None,  # Where's our AI hanging out? 
        request_timeout: Optional[int] = None  # How long before we give up? >.>
    ): 
        """
        Time to make friends with an LLM! Let's get this party started \\o/
        
        Args:
            model_name: Pick your AI companion! (or let it pick itself =P)
            temperature: Creativity dial - 0 is boring, 1 is chaos! ^_^
            ollama_url: The secret hideout of our AI friend
            request_timeout: How patient should we be? (in seconds, because time is hard x.x)
        """
        self._model_name = model_name
        self._temperature = temperature
        self._ollama_url = ollama_url
        self._request_timeout = request_timeout
        
        # Time to start our diary! (aka logging) ^_^
        self.logger = logging.getLogger('WITS.LLMInterface')
        
        # Debug mode: for when things go boom! >.>
        self.debug_enabled = False
        self.debug_config = {"log_prompts": False, "log_responses": False, "log_tokens": False}
        
        self.logger.info(f"Ready to rock with model: {self._model_name or 'default'}, temp: {self._temperature} \\o/")
        if self._ollama_url:
            self.logger.info(f"Using Ollama server at {self._ollama_url}")

    @property
    def model_name(self) -> Optional[str]:
        """Get the current model name (our AI buddy's nickname =P)"""
        return self._model_name
    
    @model_name.setter
    def model_name(self, value: str):
        """Time to give our AI a new name! ^_^"""
        self._model_name = value
    
    @property
    def temperature(self) -> float:
        """How wild is our AI feeling? Temperature check! O.o"""
        return self._temperature
    
    @temperature.setter
    def temperature(self, value: float):
        """
        Adjust the AI's creativity thermostat! 
        0.0 = Serious business mode
        1.0 = Party mode! \\o/
        """
        if not 0.0 <= value <= 1.0:
            raise ValueError("Whoa there! Temperature must be between 0.0 and 1.0 x.x")
        self._temperature = value

    async def generate_text(
        self,
        prompt: str,
        model_name: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Time to make the AI think! Let's see what it comes up with ^_^
        
        Args:
            prompt: The question we're asking our AI friend
            model_name: Which AI to chat with (optional party guest =P)
            options: Extra settings for maximum fun! \\o/
            
        Returns:
            str: What our AI friend had to say! 
        """
        start_time = time.time()
        model_to_use = model_name or self._model_name
        if not model_to_use:
            raise ValueError("Oops! We forgot to pick an AI to talk to! x.x")
        
        effective_options = {
            "temperature": self._temperature  # Setting the creativity meter! ^_^
        }
        
        # Are we in a hurry? Set a timeout! >.>
        if self._request_timeout:
            effective_options["timeout"] = self._request_timeout
        
        # Got any special requests? Add them here! =D
        if options:
            effective_options.update(options)

        # Sneaky debug mode activated! O.o
        if self.debug_enabled and self.debug_config.get('log_prompts', False):
            self.logger.debug(
                f"Here's what we're asking {model_to_use}:\n"
                f"{'='*40}\n{prompt}\n{'='*40}"
            )
        
        try:
            # Time to poke the AI and see what happens! ^_^
            generation_start = time.time()
            response = ollama.generate(
                model=model_to_use,
                prompt=prompt,
                options=effective_options
            )
            generation_time = (time.time() - generation_start) * 1000  # ms
            
            result = response.get('response', '').strip()
            
            if self.debug_enabled:
                # Time to spy on our AI's performance! O.o
                debug_info = DebugInfo(
                    timestamp=datetime.now().isoformat(),
                    component="LLMInterface",
                    action="generate_text",
                    details={
                        "model": model_to_use,
                        "prompt_length": len(prompt),
                        "response_length": len(result),
                    },
                    duration_ms=generation_time,
                    success=True
                )
                log_debug_info(self.logger, debug_info)
                
                # Let's see what our AI friend said! =D
                if self.debug_config.get('log_responses', False):
                    self.logger.debug(
                        f"Our AI buddy {model_to_use} says:\n"
                        f"{'='*40}\n{result}\n{'='*40}"
                    )
                
                # Number crunching time! x.x
                if self.debug_config.get('log_tokens', False) and 'eval_count' in response:
                    self.logger.debug(                        f"Fun stats from {model_to_use}! \\o/\n"
                        f"Brain cells used: {response['eval_count']}\n"
                        f"Think time: {response.get('eval_duration', 'Too fast to measure! =P')}"
                    )
            
            return result
            
        except Exception as e:
            error_msg = f"Uh oh! {model_to_use} got confused: {e} x.x"
            
            if self.debug_enabled:
                # Time to document our oopsie! O.o
                debug_info = DebugInfo(
                    timestamp=datetime.now().isoformat(),
                    component="LLMInterface",
                    action="generate_text",
                    details={
                        "model": model_to_use,
                        "prompt_length": len(prompt),
                        "options": effective_options
                    },
                    duration_ms=(time.time() - start_time) * 1000,
                    success=False,
                    error=str(e)
                )
                log_debug_info(self.logger, debug_info)
            
            self.logger.error(error_msg, exc_info=True)
            return f"Oops! {model_to_use} had a brain freeze >.< Details: {str(e)}"

    async def generate_structured_orchestrator_response_simple(
        self,
        prompt: str,
        model_name: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Union[OrchestratorLLMResponse, str]:
        """
        Time for some organized thinking! Let's get our AI to format things nicely ^_^
        
        We'll help our AI friend:
        1. Parse their thoughts into pretty JSON =D
        2. Give them a few retries if they mess up >.>
        3. Package everything in a nice Pydantic bow! \\o/
        
        Args:
            prompt: The big question for our AI buddy
            model_name: Which AI to think about this? (optional)
            options: Special instructions for extra pizzazz! 
            
        Returns:
            Union[OrchestratorLLMResponse, str]: Either a neat package or an "oops" message x.x
        """
        model_to_use = model_name or self._model_name
        if not model_to_use:
            raise ValueError("Help! We forgot to pick an AI to talk to! x.x")
        
        # Construct messages for the chat endpoint
        messages = [
            {
                "role": "system",
                "content": "You MUST respond with valid JSON only. No other text, explanations, or formatting outside of the JSON structure. The JSON should conform to the OrchestratorLLMResponse schema."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        effective_options = {
            "temperature": self._temperature,
            # "format": "json" # Not used with chat endpoint directly, system prompt handles JSON enforcement
        }
        
        if self._request_timeout:
            effective_options["timeout"] = self._request_timeout
        
        if options:
            effective_options.update(options)

        self.logger.info(f"Time to chat with {model_to_use}! (JSON mode via chat endpoint! \\\\o/)")
        # Reduced logging verbosity for prompt
        # self.logger.debug(f"Here\'s a peek at what we\'re asking:\\n{prompt[:300]}...\\n...\\n{prompt[-300:]}")

        max_retries = 2  # Everyone deserves a second chance! ^_^
        
        raw_json_output = ""  # Initialize for scope
        for attempt in range(max_retries):
            try:
                # Using ollama.chat instead of ollama.generate
                response = ollama.chat(
                    model=model_to_use,
                    messages=messages,
                    options=effective_options,
                    format="json" # Explicitly ask for JSON format if supported by chat
                )
                
                raw_json_output = response['message']['content'].strip()
                self.logger.info(f"Got response from {model_to_use} (attempt {attempt + 1}):\\n{raw_json_output[:300]}...")

                # Time to make it all pretty and organized! ^_^
                parsed_response = OrchestratorLLMResponse.model_validate_json(raw_json_output)
                self.logger.info(f"Yay! {model_to_use}'s response is all neat and tidy! =D")
                return parsed_response
            
            except json.JSONDecodeError as e_json:
                error_msg = f"Oopsie! {model_to_use} forgot how to JSON (attempt {attempt + 1}/{max_retries}): {e_json} x.x"
                self.logger.error(f"{error_msg}\\nRaw output:\\n{raw_json_output}")
                
                if attempt == max_retries - 1:
                    formatted_output = raw_json_output[:500] + "..." if len(raw_json_output) > 500 else raw_json_output
                    return f"Error: Our AI friend couldn't write proper JSON after {max_retries} tries! Output: {formatted_output}"
            
            except ValidationError as e_val:
                error_msg = f"Almost there! But the format isn't quite right (attempt {attempt + 1}/{max_retries}): {e_val} O.o"
                self.logger.error(f"{error_msg}\\nRaw output:\\n{raw_json_output}")
                
                if attempt == max_retries - 1:
                    formatted_output = raw_json_output[:500] + "..." if len(raw_json_output) > 500 else raw_json_output
                    return f"Error: {model_to_use} tried their best, but the response format wasn't quite right after {max_retries} attempts! Output: {formatted_output}"
            
            except Exception as e:
                error_msg = f"Unexpected plot twist! (attempt {attempt + 1}/{max_retries}): {e} >.< "
                self.logger.error(f"{error_msg}\\nRaw output:\\n{raw_json_output}", exc_info=True)
                
                if attempt == max_retries - 1:
                    return f"Error: {model_to_use} ran into a wall after {max_retries} attempts! Details: {str(e)}"
              # Quick breather before trying again! =P
            # await asyncio.sleep(1) # Consider if async sleep is needed; ollama.chat is synchronous
          # Fallback return if we somehow got here after all retries
        return "Error: Something went wrong in our AI adventure x.x"
        
async def generate_structured_orchestrator_response(
        self,
        prompt_parts: List[Union[str, Dict[str, Any]]],
        session_id: str,
    ) -> Optional[OrchestratorLLMResponse]:
        """
        Time for some organized thinking! Let's get our AI to format things nicely ^_^
        
        We'll help our AI friend:
        1. Parse their thoughts into pretty JSON =D
        2. Give them a few retries if they mess up >.>
        3. Package everything in a nice Pydantic bow! \\o/
        
        Args:
            prompt_parts: The big question for our AI buddy, in parts
            session_id: A unique identifier for this chat session
            
        Returns:
            Optional[OrchestratorLLMResponse]: A neat package or None if there was an error x.x
        """
        model_to_use = self.model_name
        if not model_to_use:
            self.logger.error(f"Model name not set in LLMInterface for session '{session_id}'.")
            return None
        
        # Create messages with enhanced system prompt for JSON
        messages = []
        # Strengthened system prompt with explicit formatting requirements:
        system_prompt_content = (
            "IMPORTANT: You are a JSON-only response generator. You MUST follow these strict rules:\n\n"
            "1. Your ENTIRE response must be a single, valid JSON object.\n"
            "2. The JSON MUST conform to the OrchestratorLLMResponse schema.\n"
            "3. DO NOT include ANY text before or after the JSON object.\n"
            "4. NO explanations, NO apologies, NO conversational text.\n"
            "5. NO markdown formatting, NO code blocks, NO backticks.\n"
            "6. Your response MUST start with '{' and end with '}'.\n"
            "7. Ensure the JSON is properly formatted with correct quotes and braces.\n\n"
            "Example of correct response format:\n"
            "{\n"
            "  \"thoughts\": \"My analysis of the situation\",\n"
            "  \"reasoning\": \"The logical steps that led to my decision\",\n"
            "  \"plan\": [\"Step 1\", \"Step 2\"],\n"
            "  \"command\": {\n"
            "    \"name\": \"example_command\",\n"
            "    \"args\": {\"key\": \"value\"}\n"
            "  }\n"
            "}\n\n"
            "Failure to follow these instructions will result in an error. ONLY RETURN VALID JSON."
        )
        messages.append({"role": "system", "content": system_prompt_content})
        
        # Add all user prompt parts
        for part in prompt_parts:
            if isinstance(part, str):
                messages.append({"role": "user", "content": part})
            elif isinstance(part, dict) and "role" in part and "content" in part:
                messages.append(part)
            else:
                messages.append({"role": "user", "content": str(part)})
        
        effective_options = {"temperature": self.temperature}
        if self._request_timeout:
            effective_options["timeout"] = self._request_timeout
        
        self.logger.debug(f"Ollama Request for session '{session_id}': Model='{model_to_use}', Messages='{json.dumps(messages, indent=2)}', Format='json', Temperature='{self.temperature}'")

        max_retries = 3
        llm_response_content = ""
        fallback_reason = None  # Track why fallback is used

        for attempt in range(max_retries):
            try:
                response = ollama.chat(
                    model=model_to_use,
                    messages=messages,
                    format="json",
                    options=effective_options
                )
                
                self.logger.debug(f"Raw Ollama Response for session '{session_id}' (attempt {attempt + 1}): {json.dumps(response, indent=2)}")
                llm_response_content = response['message']['content'].strip()
                self.logger.debug(f"LLM raw content for session '{session_id}' (attempt {attempt + 1}): '''{llm_response_content}'''")
                
                # Use our enhanced safe_json_loads function which handles conversational text
                try:
                    parsed_dict = safe_json_loads(llm_response_content, session_id=session_id)
                except Exception as e_safe_load:
                    fallback_reason = f"safe_json_loads failed: {e_safe_load}"
                    self.logger.warning(f"Fallback reason: {fallback_reason}. Raw content: '''{llm_response_content}'''")
                    parsed_dict = {}  # Default to an empty dictionary as a fallback
                
                # If we get a valid dict back (even a default one), try to validate it
                if parsed_dict:
                    try:
                        parsed_response = OrchestratorLLMResponse.model_validate(parsed_dict)
                        self.logger.info(f"Successfully parsed and validated LLM response for session '{session_id}' on attempt {attempt + 1}.")
                        return parsed_response
                    except ValidationError as e_val:
                        self.logger.warning(f"ValidationError for session '{session_id}' (attempt {attempt + 1}/{max_retries}): {e_val}. Using our best attempt at creating a valid response.")
                        
                        # Try to fix common validation issues
                        if "command" in parsed_dict and isinstance(parsed_dict["command"], dict):
                            # Ensure command has required fields
                            if "name" not in parsed_dict["command"]:
                                parsed_dict["command"]["name"] = "unknown_command" # Or some default
                            if "args" not in parsed_dict["command"]:
                                parsed_dict["command"]["args"] = {} # Or some default
                        
                        # Log fallback reason if not already set
                        if not fallback_reason:
                            fallback_reason = "ValidationError encountered during response parsing"
                        self.logger.warning(f"Fallback reason: {fallback_reason}. Attempting to fix response structure. Raw content: '''{llm_response_content}'''")
                        
                        # Add required fields if missing
                        required_fields = {"thought_process": OrchestratorLLMResponse.model_fields["thought_process"].default, 
                                           "chosen_action": OrchestratorLLMResponse.model_fields["chosen_action"].default}
                        
                        # Ensure thought_process and chosen_action are dicts
                        if "thought_process" not in parsed_dict or not isinstance(parsed_dict["thought_process"], dict):
                            parsed_dict["thought_process"] = {}
                        if "chosen_action" not in parsed_dict or not isinstance(parsed_dict["chosen_action"], dict):
                            parsed_dict["chosen_action"] = {}

                        # Populate OrchestratorThought fields
                        if "reasoning" not in parsed_dict["thought_process"]:
                             parsed_dict["thought_process"]["reasoning"] = "Fallback reasoning due to parsing error."
                        # Populate OrchestratorAction fields
                        if "action_type" not in parsed_dict["chosen_action"]:
                            parsed_dict["chosen_action"]["action_type"] = "fallback_action"


                        # Try validating again with fixed structure
                        try:
                            parsed_response = OrchestratorLLMResponse.model_validate(parsed_dict)
                            self.logger.info(f"Successfully applied fallback and validated LLM response for session '{session_id}' on attempt {attempt + 1}.")
                            return parsed_response
                        except ValidationError as e_val2:
                            self.logger.error(f"Fallback validation failed for session '{session_id}' (attempt {attempt + 1}/{max_retries}): {e_val2}. Raw content: '''{llm_response_content}'''")
                            # Fall through to retry or final error

            except json.JSONDecodeError as e_json:
                self.logger.error(f"JSONDecodeError for session '{session_id}' (attempt {attempt + 1}/{max_retries}): {e_json}. Raw content: '''{llm_response_content}'''")
            except Exception as e:
                self.logger.error(f"Generic error in generate_structured_orchestrator_response for session '{session_id}' (attempt {attempt + 1}/{max_retries}): {e}. Raw content: '''{llm_response_content}'''", exc_info=True)
            
            if attempt < max_retries - 1:
                self.logger.info(f"Retrying LLM call for session '{session_id}' (attempt {attempt + 2}/{max_retries}) with additional JSON enforcement.")
                
                # Add stronger JSON enforcement for subsequent attempts
                # Ensure we are modifying a copy or reconstructing messages to avoid issues with mutable list
                current_user_messages = [msg for msg in messages if msg["role"] == "user"]
                messages = [
                    {"role": "system", "content": system_prompt_content + "\\n\\nYOUR PREVIOUS RESPONSE WAS NOT VALID JSON. DO NOT INCLUDE ANY TEXT OUTSIDE THE JSON STRUCTURE. START WITH { AND END WITH }."},
                ] + current_user_messages
            else:
                self.logger.error(f"Failed to get valid structured response for session '{session_id}' after {max_retries} attempts. Creating fallback response.")
                # Create a fallback response structure when all attempts fail
                # This part is complex and depends on how you want to handle complete failure.
                # For now, returning None as per the original logic for failure.
                # Consider creating a minimal OrchestratorLLMResponse with error info.
                
        return None

    async def chat_completion_async(
        self,
        messages: List[Dict[str, str]],
        model_name: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None,
        format: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Asynchronously sends a chat completion request to the LLM provider.
        
        Args:
            messages: A list of message dictionaries representing the conversation history.
                Each message should have 'role' and 'content' keys.
            model_name: The name of the model to use. If None, falls back to the instance's
                default model name.
            options: Additional options to pass to the model provider. These will be merged
                with the default options set on the instance.
            format: Specifies the desired output format. If set to "json", adds a system
                message instructing the model to return valid JSON.
        
        Returns:
            A dictionary containing:
                - 'response': The model's response text
                - 'model': The name of the model used
                - 'created_at': Timestamp when the response was created
                
        Raises:
            ValueError: If no model name is provided and no default model is set.
            
        Note:
            If an exception occurs during the API call, the method will catch it,
            log the error, and return a dictionary with an 'error' key containing
            the error message.
        """
        model_to_use = model_name or self._model_name
        if not model_to_use:
            raise ValueError("Oops! We need to pick an AI to chat with! x.x")

        effective_options = {
            "temperature": self._temperature  # Setting the fun-meter! ^_^
        }

        if self._request_timeout:
            effective_options["timeout"] = self._request_timeout  # Don't leave us hanging! >.>
        
        if options:
            effective_options.update(options)

        # If we want JSON, let's make sure our AI knows! =P
        # Make a copy of messages to avoid modifying the original list if it's passed around
        processed_messages = list(messages) 
        if format == "json":
            # Prepend system message for JSON format
            processed_messages = [
                {
                    "role": "system",
                    "content": "You must respond with valid JSON only. No other text or formatting. ^_^"
                }
            ] + processed_messages
            
        try:
            # Time to have a nice chat! \\o/
            response = ollama.chat(
                model=model_to_use,
                messages=processed_messages, # Use the potentially modified list
                options=effective_options
            )
            # Package everything up nicely! =D
            return {
                "response": response['message']['content'], # Access content correctly
                "model": response['model'], # Access model correctly
                "created_at": response['created_at'], # Access created_at correctly
            }
            
        except Exception as e:
            error_msg = f"Oh no! Chat with {model_to_use} went wrong: {e} x.x"
            self.logger.error(error_msg, exc_info=True)
            return {"error": error_msg}
            
    def _construct_messages(self, prompt_parts: List[Union[str, Dict[str, Any]]]) -> List[Dict[str, str]]:
        """Constructs the message list for Ollama chat, ensuring a system prompt for JSON."""
        messages = []
        # Strengthened system prompt with explicit formatting requirements:
        system_prompt_content = (
            "IMPORTANT: You are a JSON-only response generator. You MUST follow these strict rules:\n\n"
            "1. Your ENTIRE response must be a single, valid JSON object.\n"
            "2. The JSON MUST conform to the OrchestratorLLMResponse schema.\n"
            "3. DO NOT include ANY text before or after the JSON object.\n"
            "4. NO explanations, NO apologies, NO conversational text.\n"
            "5. NO markdown formatting, NO code blocks, NO backticks.\n"
            "6. Your response MUST start with '{' and end with '}'.\n"
            "7. Ensure the JSON is properly formatted with correct quotes and braces.\n\n"
            "Example of correct response format:\n"
            "{\n"
            "  \"thoughts\": \"My analysis of the situation\",\n"
            "  \"reasoning\": \"The logical steps that led to my decision\",\n"
            "  \"plan\": [\"Step 1\", \"Step 2\"],\n"
            "  \"command\": {\n"
            "    \"name\": \"example_command\",\n"
            "    \"args\": {\"key\": \"value\"}\n"
            "  }\n"
            "}\n\n"
            "Failure to follow these instructions will result in an error. ONLY RETURN VALID JSON."
        )
        messages.append({"role": "system", "content": system_prompt_content})

        for part in prompt_parts:
            if isinstance(part, str):
                messages.append({"role": "user", "content": part})
            elif isinstance(part, dict) and "role" in part and "content" in part:
                # If a part is already a message dict, add it directly
                # (could overwrite system prompt if not careful, but current structure implies user parts)
                messages.append(part)
            else:
                # Fallback for other dict structures or types
                messages.append({"role": "user", "content": str(part)})
        return messages
